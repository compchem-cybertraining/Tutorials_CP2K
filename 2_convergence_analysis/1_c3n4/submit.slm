#!/bin/sh
#SBATCH --partition=valhalla  --qos=valhalla
#SBATCH --clusters=faculty

#SBATCH --time=02:30:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=9
#SBATCH --cpus-per-task=1
#SBATCH --mem=68000
###SBATCH --mail-user=alexeyak@buffalo.edu
echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_JOB_NODELIST="$SLURM_JOB_NODELIST
echo "SLURM_NNODES="$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR
echo "working directory="$SLURM_SUBMIT_DIR

NPROCS=`srun --nodes=${SLURM_NNODES} bash -c 'hostname' |wc -l`
echo NPROCS=$NPROCS


module load cp2k/8.1-sse

#module load openmpi/3.0.3/gcc-7.3.0
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so

if [ -n "$SLURM_CPUS_PER_TASK" ]; then
  omp_threads=$SLURM_CPUS_PER_TASK
else
  omp_threads=1
fi
export OMP_NUM_THREADS=$omp_threads



# Remember you need to have the CP2K loaded. This is done either by `module load` or by exporting the path.
# This is explained in Readme.md file

for cutoff in {100..1300..200}; do
    for ngrids in {4,,8,12}; do
       # Substituting the CUTOFF value 
       sed -i "/CUTOFF /c\     CUTOFF $cutoff" energy.inp
       # Substituting the NGRIDS value 
       sed -i "/NGRIDS /c\     NGRIDS $ngrids" energy.inp
       echo "----------$cutoff-------"
       echo "Running for cutoff $cutoff Ry and NGRIDS $ngrids..."
       mpirun -n $NPROCS cp2k.popt -i energy.inp -o OUT-conv-anal-$cutoff-$ngrids.log
       echo "Done!"
       # This command will 
       awk 'c&&!--c;/SCF WAVEFU/{c=4}' OUT-conv-anal-$cutoff-$ngrids.log | awk '{print $1 "      " $4 "      " $6}'
       # You can also use this command as well
       #cat OUT-conv-anal-$cutoff.log | grep -A 4 'SCF WAVEFUNC'
    
    done
done




